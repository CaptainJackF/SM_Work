# ASO game comment

library( dplyr)
library( data.table) # load data.table package( `fread` function ) for read csv file. ( solve question 1)
library( rJava)
library( Rwordseg)
# library( caret)

# ------- 读取文件&筛选有用信息
setwd( "D:\\R_Working_Directory\\Data\\Game_Comment")
#comment <- read.csv("aso_comments_test.csv", header = FALSE, quote = "", stringsAsFactors = FALSE)
# Question 1: Can not read special character, such as "→", into R.

comment <- fread( 
  "aso_comments2.csv", 
  sep = ",",
  stringsAsFactors = FALSE, 
  header = FALSE, # TRUE : first line contain column names.
  encoding = "unknown", # default is "unknown".
  data.table = TRUE # TRUE returns a data.table. FALSE returns a data.frame
)

# add col names. 
names( comment) <- c( "id", "Client", "Store", "Device", "Location", 
                      "Free", "Game_type", "Null_1", "Game_name", "Null_2",
                      "Null_3", "Unknow_1", "Comment_title", "User_name", 
                      "Comment", "Comment_time", "Null_5") 

# 设置key，key 在data.table 中是灵魂一般的存在，设置了 key后，会自动按照key排序
setkey( comment, Game_type) 
haskey( comment) # 检验data.table是否存在key
# 删除列, “-” 与 "!" 功能一样。
comment <- comment[ ,-c( "Null_1", "Null_2", "Null_3", "Null_5", 
                         "Client", "Store", "Device", "Location", "Free")]
# 添加新列，直接用 [] 操作， "New_Col":= function/100 添加计算后的数值或者直接添加数值/字符串
comment[ ,"Comment_date":= as.Date( Comment_time)] # dt[]操作中，不需要加$ 
dim( comment) # DF 维度

# 查看新列
# 对于dft来说，[] 中不用使用$符号即可引用列名，但直接输入列名，则输出的结果会降维(同df).
# comment[ 1:10, Comment_date] # 
comment[ 1:10, .( Comment_time, Comment_date)] # 采取list()的方式选取变量则不会， .() 是list()的简写

Wangzhe <- filter( comment, Game_name == "王者荣耀")
write.csv( Wangzhe, "王者荣耀.csv")


# ------ 打标签 > 分词 > 筛选掉短评和刷屏
## 该部分暂不做词性划分

## 赋值给新变量, 打上标签, index_code
All_Comment <- mutate( comment, index_code = 1:nrow( comment) ) 

need_sep_All <- All_Comment[ ,"Comment"]  # 提取评论数据
## 加载相关词库
installDict("王者荣耀【官方推荐】.scel","Tencent1")
installDict("腾讯游戏（词库）（第一分库）.scel","Tencent2")

sep_temp_All <- segmentCN( need_sep_All) # 分词, Takes more than 200secs.
sep_temp_ulist_All <- unlist( sep_temp_All) # unlist化，用于提取每一个元素

## 剔除短句、刷评论记录
remove_dusbin <- function( input, percent, length_of_comment){
  # 一下两个条件，满足其一，标记为 TRUE，即垃圾评论。
  ## 1. 当语句中的某个词在句中占比>= percent 
  ## 2. 当语句分词后，长度 < length_of_comment
  return( TRUE %in% (  as.vector( prop.table( table( input)) > percent) ) | sum( table( input)) < length_of_comment )
}

is_rubbish <- sapply( sep_temp_All, remove_dusbin, 0.15, 20) 

Result <- mutate( All_Comment, is_rubbish = is_rubbish) %>%
  filter( is_rubbish == FALSE) %>%
  select( -is_rubbish)

write.csv( Result, "All_Comment_Result.csv", row.names = FALSE)

# ------ 打标签 > 分词 > 筛选掉短评和刷屏 ------ END


# ------ 聚类
file_useful <- tbl_df( fread( 
  "All_Comment_Result.csv", 
  sep = ",",
  stringsAsFactors = FALSE, 
  header = TRUE, # TRUE : first line contain column names.
  encoding = "unknown", # default is "unknown".
  data.table = FALSE # TRUE returns a data.table. FALSE returns a data.frame
  )
)
## 自定义函数，删除英文&数字。
Del_Eng_Num <- function( input_text){
  return( gsub( pattern="[a-z|A-Z|0-9]", "", input_text))
}
file_useful$Comment1 <- sapply( file_useful$Comment, Del_Eng_Num)

## 抽样,抽取1/20 的数据做实验
comment_useful <- file_useful[ sample( 1:nrow( file_useful), nrow( file_useful)/20), ]

## 聚类
comment_word <- unlist( segmentCN( comment_useful$Comment1))
comment_word <- segmentCN( comment_useful$Comment1, returnType = "tm")

## 加载停用词
StopWords <-  unlist( 
  read.table(
    "D:\\R_Working_Directory\\Data\\Text\\Stopwords.txt", 
    stringsAsFactors = FALSE
  )
) 

removewords <- function( target_words, stop_words){
  target_words = target_words[ target_words%in%stop_words==FALSE]
  return( target_words)
}

## 转为TDM格式
comment.vector <- VectorSource( comment_word[1:100]) # 由文档构成向量，这里只做了分词处理
comment.corpus <- Corpus( comment.vector) # 构建动态语料库
# 去除停用词
comment.corpus <- tm_map( comment.corpus, removewords, StopWords) 
# 转换为 文档-词条矩阵（横着的变量为词）。TermDocumentMatrix 为 词条-文档矩阵，方便进行词频统计。
comment.dtm <- DocumentTermMatrix( comment.corpus, control = list( stopwords = TRUE))  
## 降维
tdm_removed <- removeSparseTerms( comment.dtm, 0.9)

# 转化为矩阵的形式
comment.matrix <- as.matrix( comment.dtm) 

k <- 5  
kmeansRes <- kmeans( comment.matrix, k) #k是聚类数  
mode( kmeansRes) #kmeansRes的内容 
names( kmeansRes)
kmeansRes$cluster #聚类结果
kmeansRes$size #每个类别下有多少条数据

# ------ 聚类 END


