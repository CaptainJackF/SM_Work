# 王者荣耀评论文本分析

library( dplyr)
library( data.table) # load data.table package( `fread` function ) for read csv file. ( solve question 1)
library( caret)

# ------  读取文件&筛选有用信息
setwd( "D:\\R_Working_Directory\\Data\\SM\\Game_Analysis")

#comment <- read.csv("aso_comments_test.csv", header = FALSE, quote = "", stringsAsFactors = FALSE)
# Question 1: Can not read special character, such as "→", into R.
comment <- fread( 
  "aso_comments2.csv", 
  sep = ",",
  stringsAsFactors = FALSE, 
  header = FALSE, # TRUE : first line contain column names.
  encoding = "unknown", # default is "unknown".
  data.table = TRUE # TRUE returns a data.table. FALSE returns a data.frame
)

# add col names. 
names( comment) <- c( "id", "Client", "Store", "Device", "Location", 
                      "Free", "Game_type", "Null_1", "Game_name", "Null_2",
                      "Null_3", "Unknow_1", "Comment_title", "User_name", 
                      "Comment", "Comment_time", "Null_5") 

# 删除列, “-” 与 "!" 功能一样。
comment <- comment[ ,-c( "Null_1", "Null_2", "Null_3", "Null_5", 
                         "Client", "Store", "Device", "Location", "Free")]
# 添加新列，直接用 [] 操作， "New_Col":= function/100 添加计算后的数值或者直接添加数值/字符串
comment[ ,"Comment_date":= as.Date( Comment_time)] # dt[]操作中，不需要加$ 
dim( comment) # DF 维度

# ------ 打标签 > 分词 > 创建于原评论相匹配的df > 分词

Wangzhe <- filter( comment, Game_name == "王者荣耀") # 筛选王者荣耀数据
Wangzhe <- mutate( Wangzhe, index_code = 1:nrow( Wangzhe) ) # 打上标签, index_code

need_sep <- Wangzhe[ ,"Comment"]  # 提取评论数据
sep_temp <- segmentCN( need_sep) # 分词
sep_temp_ulist <- unlist( sep_temp) # unlist化，用于提取每一个元素

## 剔除短句、刷评论记录
remove_dusbin <- function( input, percent){
  return( TRUE %in% (  as.vector(prop.table( table( input)) > percent) ) )
}
is_dusbin <- sapply( sep_temp, remove_dusbin, 0.4)


## 构建与原评论数据相匹配的df
num_of_words <- as.numeric( lengths( sep_temp)) # 通过分词后的 list，计算list中有多少个元素
length_of_list <- length( sep_temp) # 

element_df <- data.frame( 
  index_code = rep( 1:length_of_list, num_of_words), 
  Char = sep_temp_ulist,
  Char_num_each = rep( num_of_words, num_of_words),
  is_dusbin = rep( is_dusbin, num_of_words),
  stringsAsFactors = FALSE
  )

## 加载停用词
StopWords <-  unlist( 
  read.table(
    "D:\\R_Working_Directory\\Data\\Text\\Stopwords.txt", 
    stringsAsFactors = FALSE
  )
) 

## Method 1: Using apply and self function to define which “Char” is stopword. Takes 17 secs. Very slow.
start <- Sys.time()
element_df_1 <- element_df
removewords <- function( target_words, stop_words){
  return( target_words%in%stop_words )
}
element_df_1$is_Stop <- sapply( element_df_1$Char, removewords, StopWords)
print( Sys.time() - start)

## Method 2: Using merge and df: stopwords to match the stopword. 
start <- Sys.time()
element_df_2 <- element_df
StopWords_df <- data.frame( Char = StopWords, is_Stop = TRUE)
## element_df_2$is_Stop <- merge( element_df_2, StopWords_df, by.x = "Char",  by.y = "Char",all = TRUE) # 
print( Sys.time() - start)
rm( start)


# 词频统计
wordsNum <- tbl_df( as.data.frame( table( element_df_1[1:100,]$Char ) ) ) %>% 
  arrange( -Freq)
names( wordsNum) <- c( "Char", "Freq") # 重命名header
## write.csv( wordsNum, "Game_Analysis\\Need to classfy.csv", row.names = F) # 写出，人工将词意划分为2类
## 再读入
wordsNum_1 <- tbl_df( fread( 
  "Need to classfy.csv", 
  sep = ",",
  stringsAsFactors = FALSE, 
  header = TRUE, # TRUE : first line contain column names.
  encoding = "unknown", # default is "unknown".
  data.table = FALSE # TRUE returns a data.table. FALSE returns a data.frame
) )
# Temp <- data.frame( Class = c(-1,0,1), Class_1 = c( "消极", "中性", "积极"), Count = c(1,1,1), stringsAsFactors = FALSE)
# wordsNum_1 <- merge( wordsNum_1, Temp, by = "Class")

Change_to_Class <- function( x){
  return( as.numeric( filter( wordsNum_1, Char == x)[3] ) )
}

## Method 1: left join. 不到1秒
# element_df_1.1 <- merge( element_df_1, wordsNum_1, by = "Char") # 

## Method 2: apply & function, 将近半分钟
element_df_1.2 <- filter( element_df_1, is_Stop == FALSE)
element_df_1.2$Class <- sapply( element_df_1.2$Char, Change_to_Class)
element_df_1.2 <- mutate( element_df_1.2, Class = as.character( Class), Count = 1)

# Calculate the probability.
results <- tbl_df( element_df_1.2) %>% 
  group_by( index_code, Class) %>%
  select( index_code, Class, Count) %>%
  summarise(
    Count = sum( Count, na.rm = TRUE)
  )

length( table( results$index_code) )

library( reshape2)
x <- dcast( results, index_code ~ Class, mean, fill = 0)

x1 <- merge( Wangzhe, x, by.x = "index_code", all.x = TRUE)
write.csv( x1, "王者荣耀Result.csv", row.names = FALSE)

# ------ 打标签 > 分词 > 创建于原评论相匹配的df > 分词 ------ END



# ------ k-means 聚类
wordsNum_for_k <- filter( wordsNum, Freq >= 10)

dist_tdm_removed <- dissimilarity(tdm_removed, method = 'cosine')
hc <- hclust(dist_tdm_removed, method = 'mcquitty')
cutNum = 20
ct = cutree(hc,k=cutNum)
sink(file="result.txt")
for(i in 1:cutNum){  
  print(paste("第",i,"类： ",sum(ct==i),"个"));
  print("----------------");
  print(attr(ct[ct==i],"names"));#   print(doc[as.integer(names(ct[ct==i]))])  
  print("----------------")
}
sink()#输出结果
output=data.frame(clas=NULL,tag=NULL,text=NULL)
for(i in 1:cutNum){
  in_tag=tag[as.integer(names(ct[ct==i]))]  
  in_text=doc[as.integer(names(ct[ct==i]))]  
  cut_output=data.frame(clas=rep(i,length(in_tag)),tag=in_tag,text=in_text)  
  output=rbind(output,cut_output)
}
write.table(output,file="classification.csv",sep=",",row.names=F)
